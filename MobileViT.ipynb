{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MobileViT_week_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MobileViT networks training for ensemble model\n"
      ],
      "metadata": {
        "id": "aoQtCATGVwmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import PIL\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import librosa, librosa.display\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Activation, Reshape, Conv2DTranspose, Dropout\n",
        "from tensorflow.keras.layers import Reshape, MaxPooling2D, Softmax\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "from IPython import display\n",
        "import random\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive/')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovK3NgDfVyfl",
        "outputId": "278abaa5-603c-4958-c89a-dc2d734e5a1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd 'drive/MyDrive/DLOI_Project/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YvdbPsqan0pU",
        "outputId": "d13cd028-c8ea-481a-ad8f-f5501b7d094c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1K5OFKaoitJ_c6tsV6UYFW_Ts0E8dqdld/DLOI_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "ahudAL4ZQzAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download dataset\n",
        "# !gdown 1-2_584NonvhhTgTEO0VBZrfooKxQujnT"
      ],
      "metadata": {
        "id": "r4S5B6uaV2Em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from scipy.sparse import load_npz\n",
        "# data = load_npz('/content/drive/MyDrive/DLOI_Project/audVisIdn.npz')\n",
        "\n",
        "path_person_test='dataset/person_test.npy'\n",
        "path_person_train='dataset/person_train.npy'\n",
        "path_person_val='dataset/person_val.npy'\n",
        "\n",
        "path_imgFrames_test='dataset/imgFrames_test.npy'\n",
        "path_imgFrames_train='dataset/imgFrames_train.npy'\n",
        "path_imgFrames_val='dataset/imgFrames_val.npy'\n",
        "\n",
        "person_test = np.load(path_person_test, mmap_mode='r')\n",
        "person_train = np.load(path_person_train, mmap_mode='r')\n",
        "person_val = np.load(path_person_val, mmap_mode='r')\n",
        "\n",
        "imgFrames_test = np.load(path_imgFrames_test, mmap_mode='r')\n",
        "imgFrames_train = np.load(path_imgFrames_train, mmap_mode='r')\n",
        "imgFrames_val = np.load(path_imgFrames_val, mmap_mode='r')\n",
        "\n",
        "print('Labels:')\n",
        "print(np.shape(person_test)) \n",
        "print(np.shape(person_train)) \n",
        "print(np.shape(person_val)) \n",
        "\n",
        "print('Faces images :')\n",
        "print(np.shape(imgFrames_test)) \n",
        "print(np.shape(imgFrames_train)) \n",
        "print(np.shape(imgFrames_val)) \n",
        "\n",
        "nb_img_train, input_size, input_size, nb_channel = np.shape(imgFrames_train)\n",
        "\n",
        "download_audio_images = False\n",
        " # Set to false to do the Mel spectogram again\n",
        "\n",
        "if download_audio_images: # Load Mel spectogram directly\n",
        "  S_DB_val_path='audio/AudioImages_Val.npy'\n",
        "  S_DB_path='audio/AudioImages_Train.npy'\n",
        "  S_DB_test_path='audio/AudioImages_Test.npy'\n",
        "\n",
        "  S_DB_val = abs(np.load(S_DB_val_path, mmap_mode='r'))\n",
        "  S_DB_test = abs(np.load(S_DB_test_path, mmap_mode='r'))\n",
        "  S_DB = abs(np.load(S_DB_path, mmap_mode='r'))\n",
        "\n",
        "  print('Audio images')\n",
        "  print(np.shape(S_DB_val)) \n",
        "  print(np.shape(S_DB_test)) \n",
        "  print(np.shape(S_DB)) \n",
        "\n",
        "else: # Load Raw speeches\n",
        "\n",
        "  path_audioTrs_test='dataset/audioTrs_test.npy'\n",
        "  path_audioTrs_train='dataset/audioTrs_train.npy'\n",
        "  path_audioTrs_val='dataset/audioTrs_val.npy'\n",
        "\n",
        "  audioTrs_test = np.load(path_audioTrs_test, mmap_mode='r')\n",
        "  audioTrs_train = np.load(path_audioTrs_train, mmap_mode='r')\n",
        "  audioTrs_val = np.load(path_audioTrs_val, mmap_mode='r')  \n",
        "\n",
        "  print('Audio speech')\n",
        "  print(np.shape(audioTrs_test)) \n",
        "  print(np.shape(audioTrs_train)) \n",
        "  print(np.shape(audioTrs_val)) \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na30llz-WJwk",
        "outputId": "0d01e130-b929-4504-e916-1364dcb0b457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels:\n",
            "(950,)\n",
            "(2850,)\n",
            "(950,)\n",
            "Faces images :\n",
            "(950, 224, 224, 3)\n",
            "(2850, 224, 224, 3)\n",
            "(950, 224, 224, 3)\n",
            "Audio speech\n",
            "(950, 56000)\n",
            "(2850, 56000)\n",
            "(950, 56000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rescale the labels between 0 and 49"
      ],
      "metadata": {
        "id": "_FblpBtMpIip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GT = list(np.unique(person_train))\n",
        "new_id = [i for i in range(49)]\n",
        "id_dict = {}\n",
        "for k in range(len(GT)):\n",
        "  id_dict[GT[k]] = new_id[k]\n",
        "\n",
        "nb_class=49\n",
        "\n",
        "person_train_GT = []\n",
        "for i in range(person_train.shape[0]):\n",
        "  person_train_GT.append(id_dict[person_train[i]]) \n",
        "person_train_GT = np.stack(person_train_GT)\n",
        "output_train_class_onehot = tf.keras.utils.to_categorical(person_train_GT, nb_class)  # create one-hot encoded class\n",
        "\n",
        "person_val_GT = []\n",
        "for i in range(person_val.shape[0]):\n",
        "  person_val_GT.append(id_dict[person_val[i]]) \n",
        "person_val_GT = np.stack(person_val_GT)\n",
        "output_val_class_onehot = tf.keras.utils.to_categorical(person_val_GT, nb_class)  # create one-hot encoded class\n",
        "\n",
        "person_test_GT = []\n",
        "for i in range(person_test.shape[0]):\n",
        "  person_test_GT.append(id_dict[person_test[i]]) \n",
        "person_test_GT = np.stack(person_test_GT)\n",
        "output_test_class_onehot = tf.keras.utils.to_categorical(person_test_GT, nb_class)  # create one-hot encoded class\n",
        "   \n",
        "print('Labels GT:')\n",
        "print(np.shape(person_test_GT)) \n",
        "print(np.shape(person_train_GT)) \n",
        "print(np.shape(person_val_GT)) "
      ],
      "metadata": {
        "id": "d_T8OqClbU-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d253bb42-d37f-4aa6-91a4-374c61ab4760"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels GT:\n",
            "(950,)\n",
            "(2850,)\n",
            "(950,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not download_audio_images:\n",
        "  n_fft_opti = 2048\n",
        "  hop_length_opti = 512 \n",
        "  sr = 16000\n",
        "\n",
        "  #training \n",
        "  S_DB = []\n",
        "  for i in range(audioTrs_train.shape[0]):\n",
        "    S = librosa.feature.melspectrogram(audioTrs_train[i], sr=sr, n_fft=n_fft_opti, hop_length=hop_length_opti)\n",
        "    S_DB.append(np.stack([librosa.power_to_db(S, ref=np.max),librosa.power_to_db(S, ref=np.max),librosa.power_to_db(S, ref=np.max)], axis=2))\n",
        "\n",
        "  S_DB = np.stack(S_DB, axis = 0)\n",
        "    \n",
        "  librosa.display.specshow(S_DB[2,:,:,0], sr=sr, x_axis='time', y_axis='mel')\n",
        "  plt.colorbar(format='%+2.0f dB')\n",
        "\n",
        "  # validation\n",
        "  S_DB_val = []\n",
        "  for i in range(audioTrs_val.shape[0]):\n",
        "    S = librosa.feature.melspectrogram(audioTrs_val[i], sr=sr, n_fft=n_fft_opti, hop_length=hop_length_opti)\n",
        "    S_DB_val.append(np.stack([librosa.power_to_db(S, ref=np.max),librosa.power_to_db(S, ref=np.max),librosa.power_to_db(S, ref=np.max)], axis=2))\n",
        "\n",
        "  S_DB_val = np.stack(S_DB_val, axis = 0)\n",
        "\n",
        "  plt.figure()\n",
        "  librosa.display.specshow(S_DB_val[2,:,:,0], sr=sr, x_axis='time', y_axis='mel')\n",
        "  plt.colorbar(format='%+2.0f dB')\n",
        "\n",
        "  S_DB_test = []\n",
        "  for i in range(audioTrs_test.shape[0]):\n",
        "    S = librosa.feature.melspectrogram(audioTrs_test[i], sr=sr, n_fft=n_fft_opti, hop_length=hop_length_opti)\n",
        "    S_DB_test.append(np.stack([librosa.power_to_db(S, ref=np.max),librosa.power_to_db(S, ref=np.max),librosa.power_to_db(S, ref=np.max)], axis=2))\n",
        "\n",
        "  S_DB_test = np.stack(S_DB_test, axis = 0)\n",
        "\n",
        "  # Scale audio mel spectrogram values to match cv image range (0 - 255)\n",
        "  max_val = np.max([np.max(S_DB), np.max(S_DB_val), np.max(S_DB_test)])\n",
        "\n",
        "  S_DB = 255/max_val*S_DB # range 0 to 255\n",
        "  S_DB_val = 255/max_val*S_DB_val\n",
        "  S_DB_test = 255/max_val*S_DB_test\n",
        "else:\n",
        "  fuse_id = []\n",
        "  for i in range(S_DB.shape[0]):\n",
        "    fuse_id.append(i%3 == 0)\n",
        "  S_DB = S_DB[fuse_id]\n"
      ],
      "metadata": {
        "id": "BRnjlNf8ptsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_val = np.max([np.max(S_DB), np.max(S_DB_val), np.max(S_DB_test)])\n",
        "\n",
        "S_DB = 255/max_val*S_DB # range 0 to 255\n",
        "S_DB_val = 255/max_val*S_DB_val\n",
        "S_DB_test = 255/max_val*S_DB_test\n",
        "fuse_id = []\n",
        "for i in range(S_DB.shape[0]):\n",
        "  fuse_id.append(i%3 == 0)\n",
        "S_DB = S_DB[fuse_id]"
      ],
      "metadata": {
        "id": "yr1zuBWb7w9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_spec(x, sr):\n",
        "  S_DB = []\n",
        "  for i in range(x.shape[0]):\n",
        "    S = librosa.feature.melspectrogram(x[i], sr=sr, n_fft=2048, hop_length=512)\n",
        "    S_DB.append(np.stack([librosa.power_to_db(S, ref=np.max),librosa.power_to_db(S, ref=np.max),librosa.power_to_db(S, ref=np.max)], axis=2))\n",
        "\n",
        "  S_DB = np.stack(S_DB, axis = 0)\n",
        "  return S_DB"
      ],
      "metadata": {
        "id": "mngu2FQMOECZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "import time  \n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/f/f9/Zoorashia_elephant.jpg -O elephant.jpg\n",
        "numTry = 1000\n",
        "numParam = 25.6e6\n",
        "\n",
        "with tf.device('/gpu:0'): # if using gpu for preprocessing /gpu:0\n",
        "  model = ResNet50(weights='imagenet')  # num of params: 25.6 M\n",
        "  img_path = 'elephant.jpg'\n",
        "  img = image.load_img(img_path, target_size=(224, 224))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "\n",
        "  tic = time.perf_counter()\n",
        "  for i in range(numTry):\n",
        "    preds = model.predict(x)\n",
        "\n",
        "  toc = time.perf_counter()\n",
        "  totTime=toc-tic\n",
        "  print(\"total time:\",totTime)\n",
        "  perRunTime = totTime/numTry\n",
        "  paramSpeed=numParam/perRunTime\n",
        "  print(\"number of parameters per second:\",paramSpeed)\n",
        "  tic = time.perf_counter()\n",
        "  test_spec = preprocess_spec(audioTrs_train, 16000)\n",
        "  toc = time.perf_counter()\n",
        "  totTime=toc-tic\n",
        "  perRunTime = totTime/(audioTrs_train.shape[0]) \n",
        "  print(\"Equivalent number of parameter for mel spectrogram: {:}\".format(perRunTime*paramSpeed))\n",
        "  # time your preprocessing operation for one sample and multiply with this\n",
        "  # value to calculate parameter count equivalency \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSDMcDD_QCCB",
        "outputId": "61f6ca7e-a67c-4144-b6dc-2caa1e7c449a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-31 08:36:28--  https://upload.wikimedia.org/wikipedia/commons/f/f9/Zoorashia_elephant.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 208.80.154.240, 2620:0:861:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|208.80.154.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168370 (164K) [image/jpeg]\n",
            "Saving to: ‘elephant.jpg’\n",
            "\n",
            "\relephant.jpg          0%[                    ]       0  --.-KB/s               \relephant.jpg        100%[===================>] 164.42K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-05-31 08:36:29 (4.56 MB/s) - ‘elephant.jpg’ saved [168370/168370]\n",
            "\n",
            "total time: 66.85674916800002\n",
            "number of parameters per second: 382908237.66604936\n",
            "Equivalent number of parameter for mel spectrogram: 4754231.160716862\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Useful functions definition"
      ],
      "metadata": {
        "id": "PwBU7WTfQrZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_step_decay(epoch, lr):\n",
        "    drop_rate = 0.1\n",
        "    epochs_drop =  80.0\n",
        "    return LEARNING_RATE * math.pow(drop_rate, math.floor(epoch/epochs_drop))"
      ],
      "metadata": {
        "id": "3_w5VasI6LTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_shift(aud, GT, shift_limit, nbr_augment):\n",
        "  rolled = []\n",
        "  rolled_GT = []\n",
        "  for k in range(aud.shape[0]):\n",
        "    rolled.append(aud[k])\n",
        "    rolled_GT.append(GT[k])\n",
        "    for i in range(nbr_augment):\n",
        "      shift_rnd = random.random()\n",
        "      if i == 0:\n",
        "        shift_rnd = - shift_rnd\n",
        "      rolled.append(np.roll(aud[k], shift=int(shift_rnd * shift_limit)))# * aud.shape[0])))\n",
        "      rolled_GT.append(np.roll(GT[k], shift=int(shift_rnd * shift_limit)))# * GT.shape[0])))\n",
        "    \n",
        "  rolled = np.stack(rolled)\n",
        "  rolled_GT = np.stack(rolled_GT)\n",
        "\n",
        "  rolled = rolled#+np.random.normal(0,0.005,(rolled.shape[0],rolled.shape[1]))\n",
        "\n",
        "  return rolled, rolled_GT"
      ],
      "metadata": {
        "id": "XATazB9BMUad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_and_accuracy(history_sl):\n",
        "  # model loss\n",
        "  loss='categorical_crossentropy'\n",
        "  metrics=['accuracy']\n",
        "\n",
        "  plt.plot(history_sl.history['loss'])\n",
        "  plt.plot(history_sl.history['val_loss'])\n",
        "  plt.title('Model loss : ' + loss)\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Training', 'Validation'], loc='best')\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  # model accuracy metric\n",
        "  plt.plot(np.array(history_sl.history[metrics[0]]))\n",
        "  plt.plot(np.array(history_sl.history['val_' + metrics[0]]))\n",
        "  plt.title('Model accuracy metric : ' + metrics[0])\n",
        "  plt.ylabel('Accuracy metric')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Training', 'Validation'], loc='best')\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "uV1ZCF16phXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define MobileViT network"
      ],
      "metadata": {
        "id": "ioKK3hhh3tp_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# ViT\n",
        "!pip install -qq -U tensorflow-addons\n",
        "\n",
        "import tensorflow_addons as tfa\n",
        "import math\n",
        "\n",
        "# Setting seed for reproducibiltiy\n",
        "SEED = 42\n",
        "keras.utils.set_random_seed(SEED)\n",
        "from keras.applications import imagenet_utils\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import math\n",
        "\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# Values are from table 4.\n",
        "patch_size = 4  # 2x2, for the Transformer blocks.\n",
        "image_size = 256\n",
        "expansion_factor = 2  # expansion factor for the MobileNetV2 blocks.\n",
        "\n",
        "def conv_block(x, filters=16, kernel_size=3, strides=2):\n",
        "    conv_layer = layers.Conv2D(\n",
        "        filters, kernel_size, strides=strides, activation=tf.nn.swish, padding=\"same\"\n",
        "    )\n",
        "    return conv_layer(x)\n",
        "\n",
        "\n",
        "\n",
        "# Reference: https://git.io/JKgtC\n",
        "\n",
        "\n",
        "def inverted_residual_block(x, expanded_channels, output_channels, strides=1):\n",
        "    m = layers.Conv2D(expanded_channels, 1, padding=\"same\", use_bias=False)(x)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    if strides == 2:\n",
        "        m = layers.ZeroPadding2D(padding=imagenet_utils.correct_pad(m, 3))(m)\n",
        "    m = layers.DepthwiseConv2D(\n",
        "        3, strides=strides, padding=\"same\" if strides == 1 else \"valid\", use_bias=False\n",
        "    )(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "    m = tf.nn.swish(m)\n",
        "\n",
        "    m = layers.Conv2D(output_channels, 1, padding=\"same\", use_bias=False)(m)\n",
        "    m = layers.BatchNormalization()(m)\n",
        "\n",
        "    if tf.math.equal(x.shape[-1], output_channels) and strides == 1:\n",
        "        return layers.Add()([m, x])\n",
        "    return m\n",
        "\n",
        "\n",
        "\n",
        "# Reference:\n",
        "# https://keras.io/examples/vision/image_classification_with_vision_transformer/\n",
        "\n",
        "\n",
        "def mlp(x, hidden_units, dropout_rate):\n",
        "    for units in hidden_units:\n",
        "        x = layers.Dense(units, activation=tf.nn.swish)(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def transformer_block(x, transformer_layers, projection_dim, num_heads=2):\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization 1.\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "        # Create a multi-head attention layer.\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
        "        )(x1, x1)\n",
        "        # Skip connection 1.\n",
        "        x2 = layers.Add()([attention_output, x])\n",
        "        # Layer normalization 2.\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        # MLP.\n",
        "        x3 = mlp(x3, hidden_units=[x.shape[-1] * 2, x.shape[-1]], dropout_rate=0.1,)\n",
        "        # Skip connection 2.\n",
        "        x = layers.Add()([x3, x2])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def mobilevit_block(x, num_blocks, projection_dim, strides=1):\n",
        "    # Local projection with convolutions.\n",
        "    local_features = conv_block(x, filters=projection_dim, strides=strides)\n",
        "    local_features = conv_block(\n",
        "        local_features, filters=projection_dim, kernel_size=1, strides=strides\n",
        "    )\n",
        "\n",
        "    # Unfold into patches and then pass through Transformers.\n",
        "    num_patches = int((local_features.shape[1] * local_features.shape[2]) / patch_size)\n",
        "    non_overlapping_patches = layers.Reshape((patch_size, num_patches, projection_dim))(\n",
        "        local_features\n",
        "    )\n",
        "    global_features = transformer_block(\n",
        "        non_overlapping_patches, num_blocks, projection_dim\n",
        "    )\n",
        "\n",
        "    # Fold into conv-like feature-maps.\n",
        "    folded_feature_map = layers.Reshape((*local_features.shape[1:-1], projection_dim))(\n",
        "        global_features\n",
        "    )\n",
        "\n",
        "    # Apply point-wise conv -> concatenate with the input features.\n",
        "    folded_feature_map = conv_block(\n",
        "        folded_feature_map, filters=x.shape[-1], kernel_size=1, strides=strides\n",
        "    )\n",
        "    local_global_features = layers.Concatenate(axis=-1)([x, folded_feature_map])\n",
        "\n",
        "    # Fuse the local and global features using a convoluion layer.\n",
        "    local_global_features = conv_block(\n",
        "        local_global_features, filters=projection_dim, strides=strides\n",
        "    )\n",
        "\n",
        "    return local_global_features\n",
        "\n",
        "def create_mobilevit(num_classes=49):\n",
        "    inputs = keras.Input(shape=(256, 256, 3))\n",
        "    x = layers.Rescaling(scale=1.0 / 255)(inputs)\n",
        "\n",
        "    # Initial conv-stem -> MV2 block.\n",
        "    x = conv_block(x, filters=16)\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=16\n",
        "    )\n",
        "\n",
        "    # Downsampling with MV2 block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=16 * expansion_factor, output_channels=24, strides=2\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=24\n",
        "    )\n",
        "\n",
        "    # First MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=24 * expansion_factor, output_channels=48, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=2, projection_dim=64)\n",
        "\n",
        "    # Second MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=64 * expansion_factor, output_channels=64, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=4, projection_dim=80)\n",
        "\n",
        "    # Third MV2 -> MobileViT block.\n",
        "    x = inverted_residual_block(\n",
        "        x, expanded_channels=80 * expansion_factor, output_channels=80, strides=2\n",
        "    )\n",
        "    x = mobilevit_block(x, num_blocks=3, projection_dim=96)\n",
        "    x = conv_block(x, filters=512, kernel_size=1, strides=1)\n",
        "\n",
        "    # Classification head.\n",
        "    x = layers.GlobalAvgPool2D()(x)\n",
        "\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n"
      ],
      "metadata": {
        "id": "2FUzYE4qwN86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "def upsample_image(dataset, chosen_input_size):\n",
        "  # INTER_LINEAR or INTER_AREA\n",
        "  return [cv2.resize(image, (chosen_input_size, chosen_input_size), interpolation=cv2.INTER_AREA) for image in dataset]"
      ],
      "metadata": {
        "id": "Ps16VosVwiIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change input sizes to 256 x 256 x 3**"
      ],
      "metadata": {
        "id": "4rp-UEaM2ggD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_img = np.stack(upsample_image(imgFrames_train, 256))\n",
        "x_train_audio = np.stack(upsample_image(S_DB, 256))"
      ],
      "metadata": {
        "id": "Rqg6Q2oYwkZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_val_img = np.stack(upsample_image(imgFrames_val, 256))\n",
        "x_val_audio = np.stack(upsample_image(S_DB_val, 256))"
      ],
      "metadata": {
        "id": "h2IN-0i52eKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_img = np.stack(upsample_image(imgFrames_test, 256))\n",
        "x_test_audio = np.stack(upsample_image(S_DB_test, 256))"
      ],
      "metadata": {
        "id": "YgSU-2gKTMX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Faces and audio training from scratch"
      ],
      "metadata": {
        "id": "fn21lqm213xp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilevit_img = create_mobilevit()"
      ],
      "metadata": {
        "id": "8wHpWmp2wgBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mobilevit_audio = create_mobilevit()"
      ],
      "metadata": {
        "id": "Zh6WGWgKSwqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
        "\n",
        "import time  \n",
        "!wget https://upload.wikimedia.org/wikipedia/commons/f/f9/Zoorashia_elephant.jpg -O elephant.jpg\n",
        "numTry = 1000\n",
        "numParam = 25.6e6\n",
        "\n",
        "with tf.device('/gpu:0'): # if using gpu for preprocessing /gpu:0\n",
        "  model = ResNet50(weights='imagenet')  # num of params: 25.6 M\n",
        "  img_path = 'elephant.jpg'\n",
        "  img = image.load_img(img_path, target_size=(224, 224))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  x = preprocess_input(x)\n",
        "\n",
        "  tic = time.perf_counter()\n",
        "  for i in range(numTry):\n",
        "    preds = model.predict(x)\n",
        "\n",
        "  toc = time.perf_counter()\n",
        "  totTime=toc-tic\n",
        "  perRunTime = totTime/numTry\n",
        "  paramSpeed=numParam/perRunTime\n",
        "  tic = time.perf_counter()\n",
        "  _ = np.stack(upsample_image(imgFrames_train, 256))\n",
        "  toc = time.perf_counter()\n",
        "  totTime=toc-tic\n",
        "  perRunTime = totTime/(audioTrs_train.shape[0]) \n",
        "  print(\"Equivalent number of parameter: {:}\".format(perRunTime*paramSpeed))\n",
        "  # time your preprocessing operation for one sample and multiply with this\n",
        "  # value to calculate parameter count equivalency \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OdPng7xR96i",
        "outputId": "df2ac74b-6465-43d7-d5fd-e61413962dd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-31 08:43:17--  https://upload.wikimedia.org/wikipedia/commons/f/f9/Zoorashia_elephant.jpg\n",
            "Resolving upload.wikimedia.org (upload.wikimedia.org)... 208.80.154.240, 2620:0:861:ed1a::2:b\n",
            "Connecting to upload.wikimedia.org (upload.wikimedia.org)|208.80.154.240|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168370 (164K) [image/jpeg]\n",
            "Saving to: ‘elephant.jpg’\n",
            "\n",
            "\relephant.jpg          0%[                    ]       0  --.-KB/s               \relephant.jpg        100%[===================>] 164.42K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-05-31 08:43:17 (4.48 MB/s) - ‘elephant.jpg’ saved [168370/168370]\n",
            "\n",
            "Equivalent number of parameter: 180945.2016642424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augment_data = False\n",
        "\n",
        "def data_augmentation_audio(data):\n",
        "  b, width, height, c = np.shape(data)\n",
        "  mask_length=20\n",
        "  augmented_data=[]\n",
        "  for i in np.arange(b):\n",
        "    type_masking=random.randint(0, 1)\n",
        "\n",
        "    if type_masking == 0:\n",
        "      min_mask_width=random.randint(0, width-mask_length-1)\n",
        "      temp=[np.zeros((height, 1)) if i<min_mask_width+mask_length and i>min_mask_width else x for i, x in enumerate(data[i])]\n",
        "      augmented_data.append(temp)\n",
        "\n",
        "    else:\n",
        "      min_mask_height=random.randint(0, height-mask_length-1)\n",
        "      temp=[np.zeros((width, 1)) if i<min_mask_height+mask_length and i>min_mask_height else x for i, x in enumerate(np.transpose(data[i], (1,0,2) ))]\n",
        "      augmented_data.append(np.transpose(temp, (1,0,2)))\n",
        "  return np.array(augmented_data)\n",
        "\n",
        "if augment_data:\n",
        "  S_DB[:,:,:,0]=data_augmentation_audio(np.expand_dims(S_DB[:,:,:,0], axis=3))[:,:,:,0]\n",
        "  S_DB[:,:,:,1]=data_augmentation_audio(np.expand_dims(S_DB[:,:,:,1], axis=3))[:,:,:,0]\n",
        "  S_DB[:,:,:,2]=data_augmentation_audio(np.expand_dims(S_DB[:,:,:,2], axis=3))[:,:,:,0]"
      ],
      "metadata": {
        "id": "rRRii3DXzjMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if augment_data:\n",
        "  # Data augmentation\n",
        "  data_augmentation = keras.Sequential(\n",
        "      [\n",
        "          layers.Normalization(),\n",
        "          layers.RandomFlip(\"horizontal\"),\n",
        "          layers.RandomFlip(\"vertical\"),\n",
        "      ],\n",
        "      name=\"data_augmentation\",\n",
        "  )\n",
        "  # Compute the mean and the variance of the training data for normalization.\n",
        "  if fuse:\n",
        "    x_train_img = data_augmentation(x_train_img)"
      ],
      "metadata": {
        "id": "tBkYAm0DKOLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_smoothing_factor = 0.1\n",
        "epochs = 40\n",
        "BATCH_SIZE = 32\n",
        "LEARNING_RATE = 0.002\n",
        "optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "loss_fn = keras.losses.CategoricalCrossentropy(label_smoothing=label_smoothing_factor)\n",
        "\n",
        "\n",
        "def run_experiment_ensemble(model, x_train_ensemble,x_val_ensemble):\n",
        "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=[\"accuracy\"])\n",
        "\n",
        "    checkpoint_filepath = \"/tmp/checkpoint\"\n",
        "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "        checkpoint_filepath,\n",
        "        monitor=\"val_accuracy\",\n",
        "        save_best_only=True,\n",
        "        save_weights_only=True,\n",
        "    )\n",
        "    \n",
        "    history = model.fit(\n",
        "          x=x_train_ensemble,\n",
        "          y=output_train_class_onehot,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_val_ensemble, output_val_class_onehot),\n",
        "          callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_step_decay, verbose=1)])\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "Xfua02jSwzme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mViT_audio = run_experiment_ensemble(mobilevit_audio, x_train_audio, x_val_audio)"
      ],
      "metadata": {
        "id": "Hz5HK8u7xnsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_and_accuracy(mViT_audio)"
      ],
      "metadata": {
        "id": "0D6p8FOFC-_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mViT_img = run_experiment_ensemble(mobilevit_img, x_train_img, x_val_img)"
      ],
      "metadata": {
        "id": "5Ftk8ZyxxKUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_and_accuracy(mViT_img)"
      ],
      "metadata": {
        "id": "nc4SRTe63kDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = mobilevit_img.evaluate(x_test_img, output_test_class_onehot)\n",
        "print(f\"Validation accuracy for images: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "id": "L1mSLyvCF1W_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = mobilevit_audio.evaluate(x_test_audio, output_test_class_onehot)\n",
        "print(f\"Validation accuracy for images: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "id": "rZkDImfSF2iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load already trained network"
      ],
      "metadata": {
        "id": "GSExb-l31_Fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilevit_audio = keras.models.load_model(\"mobileAudio\")\n",
        "%cd audio/\n",
        "mobilevit_img = keras.models.load_model(\"best\")\n",
        "%cd .."
      ],
      "metadata": {
        "id": "z0Y_wLrykzKf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19017d55-de5e-4894-f937-4b0e83a9e384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1K5OFKaoitJ_c6tsV6UYFW_Ts0E8dqdld/DLOI_Project/audio\n",
            "/content/drive/.shortcut-targets-by-id/1K5OFKaoitJ_c6tsV6UYFW_Ts0E8dqdld/DLOI_Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = mobilevit_img.evaluate(x_test_img, output_test_class_onehot)\n",
        "print(f\"Validation accuracy for images: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCanbhfSBFYV",
        "outputId": "63134e0a-f7cd-4ff9-a8e7-1ffe4733804b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 5s 113ms/step - loss: 1.0891 - accuracy: 0.9147\n",
            "Validation accuracy for images: 91.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "_, accuracy = mobilevit_audio.evaluate(x_test_audio, output_test_class_onehot)\n",
        "print(f\"Validation accuracy for images: {round(accuracy * 100, 2)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTH1hdKta8LL",
        "outputId": "ce9026fe-b7b2-42db-a388-19dfdd5b8240"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30/30 [==============================] - 4s 121ms/step - loss: 1.4221 - accuracy: 0.8000\n",
            "Validation accuracy for images: 80.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ensemble model performances"
      ],
      "metadata": {
        "id": "wK0AFFBv3fPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "proba_pred_train_img = mobilevit_img.predict(x_train_img)\n",
        "proba_pred_train_audio = mobilevit_audio.predict(x_train_audio)\n",
        "pred_train_img = np.argmax(proba_pred_train_img, axis=1) \n",
        "pred_train_audio = np.argmax(proba_pred_train_audio, axis=1) \n",
        "\n",
        "pred_train_ensemble = np.argmax(np.maximum(proba_pred_train_audio, proba_pred_train_img), axis=1) \n",
        "acc = np.sum(pred_train_ensemble==person_train_GT)/len(pred_train_ensemble)*100\n",
        "print('Training : ensemble model accuracy {}'.format(acc))"
      ],
      "metadata": {
        "id": "WaakWhNYQ3cX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "095a5009-6924-431f-a389-b0f20ec7c09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test : ensemble model accuracy 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation\n",
        "proba_pred_val_img = mobilevit_img.predict(x_val_img)\n",
        "proba_pred_val_audio = mobilevit_audio.predict(x_val_audio)\n",
        "pred_val_img = np.argmax(proba_pred_val_img, axis=1) \n",
        "pred_val_audio = np.argmax(proba_pred_val_audio, axis=1) \n",
        "\n",
        "pred_val_ensemble = np.argmax(np.maximum(proba_pred_val_audio, proba_pred_val_img), axis=1) \n",
        "acc = np.sum(pred_val_ensemble==person_val_GT)/len(pred_val_ensemble)*100\n",
        "print('Validation : ensemble model accuracy {}'.format(acc))"
      ],
      "metadata": {
        "id": "rkKKKiFJRinA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2f346d-a380-4b99-ae68-3b1cf75de517"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation : ensemble model accuracy 93.57894736842105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "proba_pred_test_img = mobilevit_img.predict(x_test_img)\n",
        "proba_pred_test_audio = mobilevit_audio.predict(x_test_audio)\n",
        "pred_test_img = np.argmax(proba_pred_test_img, axis=1) \n",
        "pred_test_audio = np.argmax(proba_pred_test_audio, axis=1) \n",
        "\n",
        "pred_test_ensemble = np.argmax(np.maximum(proba_pred_test_audio, proba_pred_test_img), axis=1) \n",
        "acc = np.sum(pred_test_ensemble==person_test_GT)/len(pred_test_ensemble)*100\n",
        "print('Test : ensemble model accuracy {}'.format(acc))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPsN9jSMbsfK",
        "outputId": "f2ae5b7e-36ac-407c-d700-613968ebd1c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test : ensemble model accuracy 94.63157894736842\n"
          ]
        }
      ]
    }
  ]
}